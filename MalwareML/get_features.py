#!/usr/bin/env python3
import os
import sys
import json
import datetime
import argparse
import traceback
from collections import defaultdict

from parser import fields
from parser.utils import SkipField
from parser.parsers import FlatteningParser

DEFAULT_OUTDIR = 'features'
DEFAULT_OUTPUT_SUFFIX = '.features.json'


def section_type(section):
    if section["IMAGE_SCN_MEM_EXECUTE"]:
        return "CODE"
    elif section["IMAGE_SCN_MEM_WRITE"] and section["IMAGE_SCN_MEM_READ"]:
        return "DATA"
    elif section["IMAGE_SCN_MEM_READ"]:
        return "HEADER"
    else:
        return "OTHER"


class ReportParser(FlatteningParser):
    """
    Takes in a report and outputs a dict of features
    Inspired by Django Rest Framework's Serializers
    """
    size = fields.IntegerField(source='target.file.size', default=0)
    sig_count = fields.CountField(source='signatures', default=0)
    signatures_dict = fields.MethodField(source='signatures', default={})
    static_imports = fields.IntegerField(source='static.imported_dll_count', default=None)
    file_read_count = fields.CountField(source='behavior.summary.file_read', default=0)
    file_written_count = fields.CountField(source='behavior.summary.file_written', default=0)
    file_deleted_count = fields.CountField(source='behavior.summary.file_deleted', default=0)
    file_copied_count = fields.CountField(source='behavior.summary.file_copied', default=0)
    file_moved_count = fields.CountField(source='behavior.summary.file_moved', default=0)
    file_opened_count = fields.CountField(source='behavior.summary.file_opened', default=0)
    file_exists_count = fields.CountField(source='behavior.summary.file_exists', default=0)
    file_failed_count = fields.CountField(source='behavior.summary.file_failed', default=0)
    tcp_count = fields.CountUniqueField(source='network.tcp.dst', default=None)
    udp_count = fields.CountUniqueField(source='network.udp.dst', default=None)
    dns_queries_count = fields.CountField(source='network.dns', default=None)
    http_count = fields.MethodField(source='network.http', default=None)
    regkey_written_count = fields.CountField(source='behavior.summary.regkey_written', default=0)
    regkey_deleted_count = fields.CountField(source='behavior.summary.regkey_deleted', default=0)
    sections_count = fields.CountField(source='static.pe_sections', default=None)
    sections_dict = fields.MethodField(method_name='get_average_section_entropy_dict', source='static.pe_sections',
                                       allow_null=True, default={})
    apistats_count = fields.CountField(source='behavior.apistats', default=0)
    #API_LdrGetProcedureAddress = fields.MethodField(source='behavior.apistats', required=False)
    #API_NtProtectVirtualMemory = fields.MethodField(source='behavior.apistats', required=False)
    #API_NtAllocateVirtualMemory = fields.MethodField(source='behavior.apistats', required=False)
    # Shouldn't this be sum? Currently will be skipped if not present, which seems backwards!!
    API_LdrGetProcedureAddress = fields.PercentSumField(source='behavior.apistats', key='LdrGetProcedureAddress', required=False)
    API_NtProtectVirtualMemory = fields.PercentSumField(source='behavior.apistats', key='NtProtectVirtualMemory', required=False)
    API_NtAllocateVirtualMemory = fields.PercentSumField(source='behavior.apistats', key='NtAllocateVirtualMemory', required=False)
    packers_dict = fields.MethodField(source='static.peid_signatures', allow_null=True, default={})
    languages_dict = fields.MethodField(source='static.pe_resources', allow_null=True, default={})

    yara_signatures_dict = fields.MethodField(source='yara_signatures', allow_null=True, default={})
    clam_av = fields.IntegerField(source='clam_av_detection', default=0)
    DLLs_dict = fields.MethodField(source='static.pe_imports', allow_null=True, default={})
    imported_APIs_dict = fields.MethodField(source='static.pe_imports', allow_null=True, default={})
    windef = fields.IntegerField(default=0)


    def get_imported_APIs_dict(self, data):
        imported_APIs = {}
        for dll in data:
            for api in dll['imports']:
            	if api['name'] != None:
                    imported_APIs['IMPORTED_API_' + api['name']] = 1
        return imported_APIs

    def get_DLLs_dict(self, data):
        return {'DLL_'+x['dll']: 1 for x in data}

    def get_yara_signatures_dict(self, data):
        return {'YARA_'+x: 1 for x in data}

    def get_signatures_dict(self, data):
        return {'SIG_' + x['name']: x['value'] if 'value' in x else 1 for x in data}

    def get_packers_dict(self, data):
        return {'PACK_'+x: 1 for x in data}

    def validate_packers_dict(self, data):
        return data if data is not None else {}

    def get_average_section_entropy_dict(self, data):
        average_by_type = defaultdict(list)
        for section in data:
            average_by_type[section_type(section)].append(section['entropy'])
        return {"SEC_" + k: sum(v) / len(v) for k, v in average_by_type.items()}

    def get_max_section_entropy_dict(self, data):
        max_by_type = defaultdict(list)
        for section in data:
            max_by_type[section_type(section)].append(section['entropy'])
        return {"SEC_" + k: max(v) for k, v in max_by_type.items()}

    def get_languages_dict(self, data):
        return {x['language']: 1 for x in data}

    # TODO this seems like it could be simpler...
    def get_http_count(self, data):
        return len([c.get('port') for c in data if c.get('data')])

    def get_API_LdrGetProcedureAddress(self, data):
        ret = 0
        for v in data.values():
            val = v.get('LdrGetProcedureAddress', None)
            ret = val if val is not None else ret
        if ret is None:
            raise SkipField
        return ret

    def get_API_NtProtectVirtualMemory(self, data):
        ret = 0
        for v in data.values():
            val = v.get('NtProtectVirtualMemory', None)
            ret = val if val is not None else ret
        if ret is None:
            raise SkipField
        return ret

    def get_API_NtAllocateVirtualMemory(self, data):
        ret = 0
        for v in data.values():
            val = v.get('NtAllocateVirtualMemory', None)
            ret = val if val is not None else ret
        if ret is None:
            raise SkipField
        return ret


def printd(*args, **kwargs):
    if args_dict['verbose']:
        print('{} || '.format(datetime.datetime.now()), end='')
        print(*args, **kwargs)


def get_hashes(directory, filter_file=None, skip_files=None, check_dir=None):
    hashes = set(x.rsplit('.json', 1)[0] for x in os.listdir(directory))
    printd("Got {} hashes in dir".format(len(hashes)))
    if filter_file is not None:
        with open(filter_file) as f:
            filter_hashes = set(l.strip().rsplit('.json', 1)[0] for l in f if (l.strip() and l[0]!='#'))
        hashes = hashes.intersection(filter_hashes)
        printd("Got {} hashes from file, {} existed in dir".format(len(filter_hashes), len(hashes)))
    if skip_files is not None:
        for skip_file in skip_files:
            with open(skip_file) as f:
                skip_hashes = set(l.strip().split()[0] for l in f if (l.strip() and l[0]!='#'))
            hashes = hashes.difference(skip_hashes)
            printd("Got {} skip hashes from file, {} remaining".format(len(skip_hashes), len(hashes)))
    if check_dir is not None:
        hashes = hashes.difference(set(f.rsplit(DEFAULT_OUTPUT_SUFFIX, 1)[0] for f in os.listdir(check_dir)))
        printd('{} hashes left after skipping existing'.format(len(hashes)))

    return hashes


def load_report_with_hash(report_hash):
    try:
        with open(os.path.join(args_dict['dir'], '{}.{}'.format(report_hash, 'json'))) as f:
            return json.load(f)
    except IOError:
        pass
    except json.JSONDecodeError:
        printd('Error parsing report for {}'.format(report_hash))
        return None

    try:
        with open(os.path.join(args_dict['dir'], report_hash)) as f:
            return json.load(f)
    except IOError:
        printd('Failed to load report for hash {}'.format(report_hash))
        return None
    except json.JSONDecodeError:
        printd('Error parsing report for {}'.format(report_hash))
        return None


def get_features_from_dict(data):
    """
    Returns dict of report features. Doesn't save to file system.
    :param data: dict of cuckoo report
    """
    rep = ReportParser(data=data)
    rep.is_valid(raise_exception=True)
    return rep.validated_data


def write_error(error_hash, msg, outdir='errors'):
    errorpath = os.path.join(outdir, h + '.error')
    with open(errorpath, 'w') as f:
        print(msg, file=f)


def delete_feature_json(filepath, ignore_errors=False):
    try:
        os.remove(filepath)
    except IOError:
        msg = 'Error deleting feature json {}'.format(filepath)
        if ignore_errors:
            printd(msg)
        else:
            print(msg)
            raise


if __name__ == "__main__":
    programDescription = "Get features from cuckoo reports"

    parser = argparse.ArgumentParser(description=programDescription)

    #--Flags--
    parser.add_argument('dir', help='Directory of reports or link to reports to parse. Should be named <hash> or <hash>.json')
    parser.add_argument('-f', '--file', help='File with list of hashes to parse from directory. If absent, will parse all')
    parser.add_argument('-o', '--output-dir', default=DEFAULT_OUTDIR, help='Directory to store output file')
    parser.add_argument('-v', '--verbose', action='store_true', help='Print verbose output')
    parser.add_argument('-e', '--no-skip-existing', action='store_true', help="Don't skip hashes that already exist in output-dir")

    args = parser.parse_args()
    args_dict = vars(args)
    printd(args_dict)
    outdir = args_dict.pop('output_dir')
    errdir = '{}_errors/'.format(os.path.normpath(outdir))

    for dirname in (outdir, errdir):
        if not os.path.isdir(dirname):
            if os.path.isfile(dirname):
                print("Output dir {} exists but is a file!".format(dirname))
                sys.exit()
            os.makedirs(dirname)

    printd('Outputting to {}'.format(outdir))
    printd('Outputting errors to {}'.format(errdir))
    
    hashes = get_hashes(args_dict['dir'], filter_file=args_dict['file'], skip_files=None,
                        check_dir=outdir if not args_dict['no_skip_existing'] else None)

    for i, h in enumerate(hashes):
        data = load_report_with_hash(h)
        if data is None:
            write_error(h, 'Could not load report for hash {}'.format(h), outdir=errdir)
            continue

        outpath = os.path.join(outdir, h + DEFAULT_OUTPUT_SUFFIX)
        rep = ReportParser(data=data, output_filename=outpath)
        if not rep.is_valid(raise_exception=False):
            print('Error for hash {}, skipping'.format(h))
            write_error(h, str(rep.errors), outdir=errdir)
        else:
            try:
                rep.save()
            except:
                print('Error saving features for {}'.format(outpath))
                write_error(h, str(traceback.format_exc()), outdir=errdir)
                # If it fails to save, it will usually leave an incomplete file
                delete_feature_json(os.path.join(outdir, '{}.features.json'.format(h)), ignore_errors=True)

        if not i % 100:
            printd('Processed item {}'.format(i))

    print('Done!')
