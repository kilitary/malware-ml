import os
import pickle
import logging

from sklearn.ensemble import VotingClassifier
from keras.wrappers.scikit_learn import KerasClassifier as KerasClassifierWrapper
from keras.models import Sequential, load_model
from keras.layers import Activation, Dense, Dropout
from keras.layers.normalization import BatchNormalization
from keras.callbacks import EarlyStopping
from keras.optimizers import SGD
from keras.utils.np_utils import to_categorical
from keras import backend as K
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session

from classifier import MultiClassifier

logger = logging.getLogger(__name__)

if os.environ.get('MML_KERAS_ALLOW_GROWTH', '0') != '0':
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    set_session(tf.Session(config=config))

## in get_best_clf_params
## cv_clf.fit(feature_matrix, to_categorical(report_truths))
# TODO not sure why we need to_categorical above, we don't need it for fit or score...

# TODO early stopping should be true/false and this should be validation split
# Or maybe we should just always have validation split (nah, not good for SW)
#KERAS_EARLY_STOPPING = float(os.environ.get('MML_KERAS_EARLY_STOPPING', '0.0'))  # Sets the validation split


def build_mlp_model(feat_len, *args, **kwargs):
    model = Sequential()
    model.add(Dense(feat_len, input_dim=feat_len, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(feat_len, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='softmax'))

    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model

def build_deep1_model(feat_len, *args, **kwargs):
    model = Sequential()
    model.add(Dense(feat_len, input_dim=feat_len, kernel_initializer='uniform', activation='relu'))
    # TODO remove kernel_initializer above
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))
    model.add(Dense(500, activation='relu'))
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))
    model.add(Dense(500, activation='relu'))
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))
    model.add(Dense(500, activation='relu'))
    #model.add(Dropout(0.1))
    model.add(Dense(2, activation='softmax'))

    sgd = SGD(lr=0.001, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model

def build_deep1exp_model(feat_len, *args, **kwargs):
    model = Sequential()
    # TODO remove kernel_initializer below
    model.add(Dense(feat_len, input_dim=feat_len, kernel_initializer='uniform'))
    model.add(Activation('relu'))
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))

    model.add(Dense(500))
    model.add(Activation('relu'))
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))

    model.add(Dense(500))
    model.add(Activation('relu'))
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))

    model.add(Dense(500))
    model.add(Activation('relu'))
    #model.add(Dropout(0.1))

    model.add(Dense(2, activation='softmax'))

    sgd = SGD(lr=0.001, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model

def build_deep2_model(feat_len, *args, **kwargs):
    model = Sequential()
    model.add(Dense(feat_len, input_dim=feat_len, kernel_initializer='uniform', activation='relu'))
    model.add(BatchNormalization())
    # TODO remove kernel_initializer above
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))
    model.add(Dense(500, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))
    model.add(Dense(500, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))
    model.add(Dense(500, activation='relu'))
    model.add(BatchNormalization())
    #model.add(Dropout(0.1))
    model.add(Dense(2, activation='softmax'))

    sgd = SGD(lr=0.001, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model

def build_exp_model(feat_len, *args, **kwargs):
    model = Sequential()
    model.add(Dense(feat_len, input_dim=feat_len, activation='relu'))
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))
    model.add(Dense(500, activation='relu'))
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))
    model.add(Dense(500, activation='relu'))
    model.add(Dropout(0.1))
    #model.add(Dropout(0.5))
    model.add(Dense(500, activation='relu'))
    #model.add(Dropout(0.1))
    model.add(Dense(2, activation='softmax'))

    sgd = SGD(lr=0.001, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model


class KerasClassifier(MultiClassifier):
    KERAS_MODEL_NAME = 'model.h5'
    model_items = [x for x in MultiClassifier.model_items if x != 'clf']  # Classifier saved separately for Keras

    type_map = dict(
        MultiClassifier.type_map,
        epochs=int,
        batch_size=int,
        patience=int,
        ensemble_voting=str,
        validation_split=float,
        early_stopping=bool,
    )

    classifiers = {
        'mlp': {
            'build_fn': build_mlp_model,
        },
        'deep1': {
            'build_fn': build_deep1_model,
        },
        'deep1exp': {
            'build_fn': build_deep1exp_model,
        },
        'exp': {
            'build_fn': build_exp_model,
        },
        'ensemble': {
            'type': 'ensemble',
        }
    }

    def __init__(self, classifier_name='mlp', epochs=100, batch_size=128, validation_split=0.0, ensemble_voting='soft',
                 early_stopping=False, patience=2, **kwargs):
        super().__init__(classifier_name, **kwargs)

        logging.info('Initializing Keras classifier {}'.format(classifier_name))
        self.epochs = epochs
        self.batch_size = batch_size
        self.patience = patience
        self.early_stopping = early_stopping
        self.validation_split = validation_split
        self.ensemble_voting = ensemble_voting

        if self.early_stopping and self.validation_split == 0.0:
            raise ValueError('If early stopping is set, validation split must be non-zero')

    def _post_loop_cleanup(self):
        K.clear_session()

    def load_model(self, model_dir):
        super().load_model(model_dir)
        self.clf = KerasClassifierWrapper(lambda: None)
        self.clf.model = load_model(os.path.join(model_dir, self.KERAS_MODEL_NAME))
        with open(os.path.join(model_dir, 'clf.classes_.pkl'), 'rb') as f:
            self.clf.classes_ = pickle.load(f)

    def save_model(self, output_dir):
        super().save_model(output_dir)
        self.clf.model.save(os.path.join(output_dir, self.KERAS_MODEL_NAME))
        with open(os.path.join(output_dir, 'clf.classes_.pkl'), 'wb') as f:
            pickle.dump(self.clf.classes_, f)

    def _build_ensemble(self, feat_len):
        class SelfCleaningKerasClassifierWrapper(KerasClassifierWrapper):
            def predict(self, *args, **kwargs):
                resp = super().predict(*args, **kwargs)
                K.clear_session()
                return resp

        clf1 = SelfCleaningKerasClassifierWrapper(build_fn=build_mlp_model, feat_len=feat_len,
                                      epochs=self.epochs, batch_size=self.batch_size, shuffle=True)
        clf2 = SelfCleaningKerasClassifierWrapper(build_fn=build_mlp_model, feat_len=feat_len,
                                      epochs=self.epochs, batch_size=self.batch_size, shuffle=True)
        clf3 = SelfCleaningKerasClassifierWrapper(build_fn=build_mlp_model, feat_len=feat_len,
                                      epochs=self.epochs, batch_size=self.batch_size, shuffle=True)

        return VotingClassifier(estimators=[('kc1', clf1), ('kc2', clf2), ('kc3', clf3)], voting=self.ensemble_voting)

    def _build_classifier(self, *args, feature_matrix, **kwargs):
        #return build_keras_model(feat_len=feat_len, epochs=self.epochs, batch_size=self.batch_size, shuffle=True)
        if self.classifier_dict.get('type', None) == 'ensemble':
            return self._build_ensemble(feat_len=len(feature_matrix[0]))
        return KerasClassifierWrapper(build_fn=self.classifier_dict['build_fn'], feat_len=len(feature_matrix[0]),
                                      epochs=self.epochs, batch_size=self.batch_size,
                                      validation_split=self.validation_split,
                                      shuffle=True)

    def _get_param_grid(self):
        return {
            'kerasclassifier__batch_size': [32, 64, 128],
            'kerasclassifier__epochs': [50, 100, 250, 500],
            'kerasclassifier__verbose': [0]
        }

    def _get_fit_params(self):
        """
        Additional params to use when calling 'fit'
        Note: The classifier will be in a pipeline, name params appropriately
        :return: dict of 'param': <value_to_set>. Ex {'kerasclassifier__verbose': 1)
        """
        if self.early_stopping:
            callbacks = [EarlyStopping(patience=self.patience),]
            return {
                'callbacks': callbacks,
            }
        else:
            return {}
