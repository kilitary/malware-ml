#!/usr/bin/env python3
import importlib
import os
import sys
import json
import logging
import argparse
import numpy as np
from collections import defaultdict, Iterable

from sklearn.metrics import auc, roc_curve, f1_score, log_loss

CATEGORY_BENIGN = 0
CATEGORY_MALICIOUS = 1


def stats(results, threshold=None):
    probabilities, truths = zip(*[(v['prediction'], v['truth']) for v in results.values()])
    probabilities = np.array(probabilities)
    truths = np.array(truths)

    accuracy(truths, probabilities)
    
    # Compute ROC curve and area the curve
    fpr, tpr, thresholds = roc_curve(truths[:], probabilities[:, 1])
    roc_auc = auc(fpr, tpr)

    print('Auc: {}'.format(roc_auc))

    for i, (x, y, thresh) in enumerate(zip(fpr, tpr, thresholds)):
        if x > 0.01:
            print('-----')
            print('At {}% fp, tpr {}% Threshold {}'.format(fpr[i-1]*100, tpr[i-1]*100, thresholds[i-1]))
            print('At {}% fp, tpr {}% Threshold {}'.format(x*100, y*100, thresh))
            print('At {}% fp, tpr {}% Threshold {}'.format(fpr[i+1]*100, tpr[i+1]*100, thresholds[i+1]))
            break

    if threshold is not None:
        pos_count = sum(1 for prob in probabilities if prob[1] > threshold)
        fp =  pos_count / len(probabilities)
        print('For a threshold of {} fp is {}%'.format(threshold, fp*100))

    with open('points.txt', 'w') as f:
        for x, y, thresh in zip(fpr, tpr, thresholds):
            f.write('{} {} {}\n'.format(x, y, thresh))

    return fpr, tpr, roc_auc


def accuracy(truths, predictions, indices=None, level=logging.WARNING):
    """
    :return: accuracy, false negative rate, false positive rate
    """
    if indices is not None:
        truths = truths[indices]
    assert len(predictions) == len(truths), 'Predictions and truths must be the same length'

    pred_probs = predictions
    if isinstance(predictions[0], Iterable):  # If this is class probabilities, convert to class number
        predictions = [np.argmax(p) for p in predictions]
    else:
        print('ERROR: Need probabilities not just predictions!')

    l = list(zip(predictions, truths))
    correct = sum(1 for p, t in l if p == t)
    fpos = sum(1 for p, t in l if p != t and t == CATEGORY_BENIGN)  # False Positives
    fneg = sum(1 for p, t in l if p != t and t == CATEGORY_MALICIOUS)  # False Negatives

    n = len(predictions)
    npos = sum(1 for t in truths if t == CATEGORY_MALICIOUS)
    nneg = sum(1 for t in truths if t == CATEGORY_BENIGN)

    if npos == 0 or nneg == 0:
        return

    accuracy = correct/n
    fnegrate, fposrate = fneg/npos, fpos/nneg

    print('----')
    print("Overall Accuracy: {:.6}".format(100*accuracy))
    print("False Neg: {:.4}% - {}/{}, False Pos: {:.4}% - {}/{}".format(
        100 * fnegrate, fneg, npos,
        100 * fposrate, fpos, nneg
    ))
    print('F1: {}'.format(f1_score(truths, predictions)))
    print('Logloss: {}'.format(log_loss(truths, pred_probs)))
    return accuracy, fnegrate, fposrate


def plot_roc(fpr, tpr, roc_auc):
    # Based on http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py
    import matplotlib.pyplot as plt  # Imported locally to reduce dependencies

    color = 'cyan'
    line_weight = 2

    plt.plot(fpr, tpr, lw=line_weight, color=color,
             label='ROC (area = %0.2f)' % (roc_auc))
    
    plt.xlim([0.005, 0.05])
    plt.ylim([0.90, 1.00])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()


# ----- Setup Functions -----
def build_parser():
    """
    Build the argparse parser.
    :return: argparse.ArgumentParser instance
    """
    parser = argparse.ArgumentParser(description='Get stats and plot ROC curve from results.json and truths')
    parser = argparse.ArgumentParser(add_help=True)
    parser.add_argument('--no-plot', action='store_true', help="Don't plot, just output stats")
    parser.add_argument('--threshold', nargs=1, type=float, help='Threshold to print fp/detection at')
    parser.add_argument('results-file', help='Results.json')
    parser.add_argument('truths-file', help='File with truths for hashes. Each line should be "<hash> <0|1>"')

    return parser


def main(argv):
    # Command line args
    cargs = build_parser().parse_args(argv[1:])
    cargs = vars(cargs)  # Convert to dict
    print(cargs)

    with open(cargs.pop('results-file')) as f:
        results = json.load(f)

    with open(cargs.pop('truths-file')) as f:
        truths = {l.split()[0].strip(): int(l.split()[1].strip()) for l in f}

    for k, v in results.items():
        v['truth'] = truths[k]

    fpr, tpr, auc = stats(results, threshold=cargs.get('threshold', None))
    
    if not cargs.pop('no_plot'):
        plot_roc(fpr, tpr, auc)


if __name__ == '__main__':
    main(sys.argv)
